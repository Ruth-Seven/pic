# Keraså’ŒTensorFlowçš„æŒ‡å—

# Kerasç¯‡

æ²¡æœ‰ä»€ä¹ˆæ¯”å®˜ç½‘æ›´èƒ½è¯´æ˜Kerasçš„ç®€å•ä¹‹å¤„äº† [å®˜ç½‘](https://keras.io/zh)

# åŸºæœ¬äº†è§£

## æŒ‡å¯¼åŸåˆ™

- **ç”¨æˆ·å‹å¥½ã€‚** Keras æ˜¯ä¸ºäººç±»è€Œä¸æ˜¯ä¸ºæœºå™¨è®¾è®¡çš„ APIã€‚å®ƒæŠŠç”¨æˆ·ä½“éªŒæ”¾åœ¨é¦–è¦å’Œä¸­å¿ƒä½ç½®ã€‚Keras éµå¾ªå‡å°‘è®¤çŸ¥å›°éš¾çš„æœ€ä½³å®è·µï¼šå®ƒæä¾›ä¸€è‡´ä¸”ç®€å•çš„ APIï¼Œå°†å¸¸è§ç”¨ä¾‹æ‰€éœ€çš„ç”¨æˆ·æ“ä½œæ•°é‡é™è‡³æœ€ä½ï¼Œå¹¶ä¸”åœ¨ç”¨æˆ·é”™è¯¯æ—¶æä¾›æ¸…æ™°å’Œå¯æ“ä½œçš„åé¦ˆã€‚
- **æ¨¡å—åŒ–ã€‚** æ¨¡å‹è¢«ç†è§£ä¸ºç”±ç‹¬ç«‹çš„ã€å®Œå…¨å¯é…ç½®çš„æ¨¡å—æ„æˆçš„åºåˆ—æˆ–å›¾ã€‚è¿™äº›æ¨¡å—å¯ä»¥ä»¥å°½å¯èƒ½å°‘çš„é™åˆ¶ç»„è£…åœ¨ä¸€èµ·ã€‚ç‰¹åˆ«æ˜¯ç¥ç»ç½‘ç»œå±‚ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€åˆå§‹åŒ–æ–¹æ³•ã€æ¿€æ´»å‡½æ•°ã€æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ƒä»¬éƒ½æ˜¯å¯ä»¥ç»“åˆèµ·æ¥æ„å»ºæ–°æ¨¡å‹çš„æ¨¡å—ã€‚
- **æ˜“æ‰©å±•æ€§ã€‚** æ–°çš„æ¨¡å—æ˜¯å¾ˆå®¹æ˜“æ·»åŠ çš„ï¼ˆä½œä¸ºæ–°çš„ç±»å’Œå‡½æ•°ï¼‰ï¼Œç°æœ‰çš„æ¨¡å—å·²ç»æä¾›äº†å……è¶³çš„ç¤ºä¾‹ã€‚ç”±äºèƒ½å¤Ÿè½»æ¾åœ°åˆ›å»ºå¯ä»¥æé«˜è¡¨ç°åŠ›çš„æ–°æ¨¡å—ï¼ŒKeras æ›´åŠ é€‚åˆé«˜çº§ç ”ç©¶ã€‚
- **åŸºäº Python å®ç°ã€‚** Keras æ²¡æœ‰ç‰¹å®šæ ¼å¼çš„å•ç‹¬é…ç½®æ–‡ä»¶ã€‚æ¨¡å‹å®šä¹‰åœ¨ Python ä»£ç ä¸­ï¼Œè¿™äº›ä»£ç ç´§å‡‘ï¼Œæ˜“äºè°ƒè¯•ï¼Œå¹¶ä¸”æ˜“äºæ‰©å±•ã€‚

------

## å¿«é€Ÿå¼€å§‹ï¼š30 ç§’ä¸Šæ‰‹ Keras

Keras çš„æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯ **model**ï¼Œä¸€ç§ç»„ç»‡ç½‘ç»œå±‚çš„æ–¹å¼ã€‚æœ€ç®€å•çš„æ¨¡å‹æ˜¯ [Sequential é¡ºåºæ¨¡å‹](https://keras.io/getting-started/sequential-model-guide)ï¼Œå®ƒç”±å¤šä¸ªç½‘ç»œå±‚çº¿æ€§å †å ã€‚å¯¹äºæ›´å¤æ‚çš„ç»“æ„ï¼Œä½ åº”è¯¥ä½¿ç”¨ [Keras å‡½æ•°å¼ API](https://keras.io/getting-started/functional-api-guide)ï¼Œå®ƒå…è®¸æ„å»ºä»»æ„çš„ç¥ç»ç½‘ç»œå›¾ã€‚

`Sequential` æ¨¡å‹å¦‚ä¸‹æ‰€ç¤ºï¼š

```
from keras.models import Sequential

model = Sequential()
```

å¯ä»¥ç®€å•åœ°ä½¿ç”¨ `.add()` æ¥å †å æ¨¡å‹ï¼š

```
from keras.layers import Dense

model.add(Dense(units=64, activation='relu', input_dim=100))
model.add(Dense(units=10, activation='softmax'))
```

åœ¨å®Œæˆäº†æ¨¡å‹çš„æ„å»ºå, å¯ä»¥ä½¿ç”¨ `.compile()` æ¥é…ç½®å­¦ä¹ è¿‡ç¨‹ï¼š

```
model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
```

å¦‚æœéœ€è¦ï¼Œä½ è¿˜å¯ä»¥è¿›ä¸€æ­¥åœ°é…ç½®ä½ çš„ä¼˜åŒ–å™¨ã€‚Keras çš„æ ¸å¿ƒåŸåˆ™æ˜¯ä½¿äº‹æƒ…å˜å¾—ç›¸å½“ç®€å•ï¼ŒåŒæ—¶åˆå…è®¸ç”¨æˆ·åœ¨éœ€è¦çš„æ—¶å€™èƒ½å¤Ÿè¿›è¡Œå®Œå…¨çš„æ§åˆ¶ï¼ˆç»ˆæçš„æ§åˆ¶æ˜¯æºä»£ç çš„æ˜“æ‰©å±•æ€§ï¼‰ã€‚

```
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))
```

ç°åœ¨ï¼Œä½ å¯ä»¥æ‰¹é‡åœ°åœ¨è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè¿­ä»£äº†ï¼š

```
# x_train å’Œ y_train æ˜¯ Numpy æ•°ç»„ -- å°±åƒåœ¨ Scikit-Learn API ä¸­ä¸€æ ·ã€‚
model.fit(x_train, y_train, epochs=5, batch_size=32)
```

æˆ–è€…ï¼Œä½ å¯ä»¥æ‰‹åŠ¨åœ°å°†æ‰¹æ¬¡çš„æ•°æ®æä¾›ç»™æ¨¡å‹ï¼š

```
model.train_on_batch(x_batch, y_batch)
```

åªéœ€ä¸€è¡Œä»£ç å°±èƒ½è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š

```
loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)
```

æˆ–è€…å¯¹æ–°çš„æ•°æ®ç”Ÿæˆé¢„æµ‹ï¼š

```
classes = model.predict(x_test, batch_size=128)
```





# åŸºç¡€å‡½æ•°





## layers



`keras.layers.Flatten(input_shape=(28, 28))` æŠŠè¾“å…¥çš„äºŒç»´æ•°æ®è½¬åŒ–ä¸ºï¼ˆï¼‰ã€‚28 * 28ï¼Œï¼‰ çš„Tsã€‚

å¦‚

```python
 # Flattenå±‚å°†é™¤ç¬¬ä¸€ç»´ï¼ˆbatch_sizeï¼‰ä»¥å¤–çš„ç»´åº¦å±•å¹³
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(10, activation=tf.nn.softmax)
])
```



## loss





## model





```python
#è®­ç»ƒæ¨¡å‹
model.compile(optimizer=tf.keras.optimizers.SGD(0.1),              loss = 'sparse_categorical_crossentropy',              		metrics=['accuracy']) model.fit(x_train,y_train,epochs=5,batch_size=256)

#æ¯”è¾ƒæµ‹è¯•æ•°æ®ä¸Šçš„è¡¨ç°
test_loss, test_acc = model.evaluate(x_test, y_test)
print('Test Acc:',test_acc)



```

â€‹	

## è½¬åŒ–

`to_categorical` æœ‰ç‚¹åƒ`tf.one-shot`

```
keras.utils.to_categorical(y, num_classes=None, dtype='float32')
```

Converts a class vector (integers) to binary class matrix.

E.g. for use with categorical_crossentropy.

**Arguments**

- **y**: class vector to be converted into a matrix (integers from 0 to num_classes).
- **num_classes**: total number of classes.
- **dtype**: The data type expected by the input, as a string (`float32`, `float64`, `int32`...)

**Returns**

A binary matrix representation of the input. The classes axis is placed last.

**Example**

```python
# Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:
> labels
array([0, 2, 1, 2, 0])
# `to_categorical` converts this into a matrix with as many
# columns as there are classes. The number of rows
# stays the same.
> to_categorical(labels)
array([[ 1.,  0.,  0.],
       [ 0.,  0.,  1.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.],
       [ 1.,  0.,  0.]], dtype=float32)
```



# å…¶ä»–Kerasä½¿ç”¨ç»†èŠ‚

**1.æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µè¯­å¥ï¼ˆLinuxï¼‰**

```text
# 1ç§’é’Ÿåˆ·æ–°ä¸€æ¬¡
watch -n 1 nvidia-smi
```

**2.æŒ‡å®šæ˜¾å¡**

```text
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
```

è¿™é‡ŒæŒ‡å®šäº†ä½¿ç”¨ç¼–å·ä¸º2çš„GPUï¼Œå¤§å®¶å¯ä»¥æ ¹æ®éœ€è¦å’Œå®é™…æƒ…å†µæ¥æŒ‡å®šä½¿ç”¨çš„GPU

**3.GPUå¹¶è¡Œ**

```text
from model import unet
G = 3 # åŒæ—¶ä½¿ç”¨3ä¸ªGPU
with tf.device("/cpu:0"):
    M = unet(input_rows, input_cols, 1)
model = keras.utils.training_utils.multi_gpu_model(M, gpus=G)
model.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics =     ['accuracy'])
model.fit(X_train, y_train,
        batch_size=batch_size*G, epochs=nb_epoch, verbose=0, shuffle=True,
        validation_data=(X_valid, y_valid))
model.save_weights('/path_to_save/model.h5')
```

**4. æŸ¥çœ‹ç½‘ç»œç»“æ„**

```python
print (model.summary())
```

æ•ˆæœå¦‚å›¾:

![img](./v2-629947404c3f96cd69462053c0469583_720w.jpg)

**5.ä¿å­˜ç½‘ç»œç»“æ„å›¾**

```python
# ä½ è¿˜å¯ä»¥ç”¨plot_model()æ¥è®²ç½‘ç»œä¿å­˜ä¸ºå›¾ç‰‡
plot_model(my_model, to_file='my_vgg16_model.png')
```

# TensorFlowç¯‡



TensorFLowæœ€å¤§çš„ä¼˜ç‚¹åœ¨äºæ¨¡å—åŒ–çš„è®¾è®¡æå¤§åœ°å¢å¼ºäº†æ¨¡å‹æ„å»ºçš„çµæ´»æ€§ï¼Œä½†æ˜¯åŒæ—¶ä¹Ÿç»™æ–°æ‰‹å¸¦äº†éš¾ä»¥ç»¼åˆç†è§£çš„éšœç¢ã€‚

[æ•™ç¨‹æŒ‡å¼•](https://github.com/machinelearningmindset/)







# TensorFlow1





## åŸºç¡€çŸ¥è¯†

### `placeholder `

 placeholder ä¸­çš„ Noneèµ·åˆ°äº†è¯´æ˜å‘é‡ç»´åº¦ï¼Œä½†éšå«å‘é‡æŸä¸€ç»´åº¦å¤§å°çš„åŠŸèƒ½ ,  é€šå¸¸batch_sizeå¯ä»¥æŒ‡å®šNoneï¼Œä»¥ä¾¿è°ƒæ•´æ ·æœ¬é‡å¤§å°

Passing None to a shape argument of a tf.placeholder tells it simply that that dimension is unspecified, and to infer that dimension from the tensor you are feeding it during run-time (when you run a session). Only some arguments (generally the batch_size argument) can be set to None since Tensorflow needs to be able to construct a working graph before run time. This is useful for when you don't want to specify a batch_size before run time.

## `.Contrib`

contribä¸€èˆ¬æ¥è¯´å°±æ˜¯é‚£äº›ç¤¾åŒºå¼€å‘çš„åŠŸèƒ½ï¼Œå…¶èƒ½åŠ›å’Œä»£ç è¡¨ç°éƒ½æ˜¯ä¸é”™çš„ï¼Œä½†æ˜¯å®˜æ–¹å¼€å‘å›¢é˜Ÿä¸ä¸€å®šä¼šæŠŠå®ƒåŠ å…¥åˆ°æœªæ¥ç‰ˆæœ¬ä¸­ï¼Œç”šè‡³å¯èƒ½ç§»é™¤æ‰ï¼ˆæ¯”å¦‚tf2.0ï¼‰ã€‚

In general, `tf.contrib` contains *contrib*uted code. It is meant to contain features and contributions that eventually should get merged into core TensorFlow, but whose interfaces may still change, or which require some testing to see whether they can find broader acceptance.

The code in `tf.contrib` isn't supported by the Tensorflow team. It is included in the hope that it is helpful, but it might change or be removed at any time; there are no guarantees.

The source of `tf.contrib.layers.sparse_column_with_hash_bucket` can be found at







# TensorFlow2

## åŸºç¡€å‘½ä»¤é€ŸæŸ¥

è®°tsï¼ŒTsä¸ºTensorå®ä¾‹

`tf.constant(X)` åˆ›å»ºä¸€ä¸ªTensorå®ä¾‹ï¼ŒXå¯ä»¥ä¸ºlistï¼ŒTensorï¼ŒNarrayã€‚

> ä¹Ÿå¯ä»¥ä½¿ç”¨ np.array(Ts)æ¥å§Tså˜æˆNa

`tf.reshape(x,(3,4))` è¿”å›reshapeè¿‡åçš„xçš„Tsã€‚è¿™æ¡å‘½ä»¤ä¹Ÿå¯ä»¥æ¢æˆ`tf.reshape(x,(-1,4))`å› ä¸ºå…ƒç´ çš„ä¸ªæ•°æ˜¯å›ºå®šçš„ï¼Œæ‰€ä»¥è¿™ä¸ªç»´åº¦çš„æ•°é‡å¯ä»¥æ¨æ–­å‡ºæ¥ã€‚

`tf.zeros(shape_tuple)` è¿”å›ä¸€ä¸ªæ–°çš„é›¶Tsã€‚

`tf.ones( shape_tuple)` è¿”å›ä¸€ä¸ªæ–°çš„å…ƒç´ å…¨ä¸º1çš„Tsã€‚

`tf.random.normal(shape=[3,4], mean=0, stddev=1)` è¿”å›ä¸€ä¸ªå…ƒç´ ä¸ºéšæœºæ­£å¤ªåˆ†å¸ƒçš„Tsã€‚

`tf.boolean_mask( tensor, mask, axis=0)`  æ ¹æ®æ©ç çš„Trueæˆ–Falseæ¥é€‰æ‹©tensorçš„å¯¹åº”axisä¸Šçš„æ•°æ®æ˜¯å¦ç•™å­˜ã€‚

ä¸€èˆ¬æ¥è¯´æ¥è¯´ï¼Œä¸¤ä¸ªå‘é‡çš„è‡³å°‘è¦æœ‰Kä¸ªï¼ˆK>=1ï¼‰ç»´åº¦çš„shapeç›¸åŒã€‚

```python
# 2-D example
tensor = [[1, 2], [3, 4], [5, 6]]
mask = np.array([True, False, True])
boolean_mask(tensor, mask)  # [[1, 2], [5, 6]]
```

`tf.one_hot(indices, depth)` è¿”å›ä¸€ä¸ª len(indices) * depth çš„one-hot çŸ©é˜µã€‚

```python
#one_hotå®ä¾‹ï¼šå®ç°ä¸€ä¸ª äº¤å‰ç†µå‡½æ•°ï¼ˆyï¼š K*N...*1ï¼‰
#return ï¼š K*N...*1
def cross_entropy(y_hat, y):
  
    y = tf.cast(tf.reshape(y, shape=[-1, 1]),dtype=tf.int32)
    y = tf.one_hot(y, depth=y_hat.shape[-1])
    y = tf.cast(tf.reshape(y, shape=[-1, y_hat.shape[-1]]),dtype=tf.int32)
    return -tf.math.log(tf.boolean_mask(y_hat, y)+1e-8)
```

`tf.argmax(ts, axis=1)` è¿”å›ä¸€è¡Œä¸­çš„æœ€å¤§å€¼çš„ä¸‹æ ‡ã€‚

```python
#å®ç°å®šä¹‰ä¸€ä¸ªAccuracy
# æè¿°,å¯¹äºtensorflow2ä¸­ï¼Œæ¯”è¾ƒçš„åŒæ–¹å¿…é¡»ç±»å‹éƒ½æ˜¯intå‹ï¼Œæ‰€ä»¥è¦å°†è¾“å‡ºå’Œæ ‡ç­¾éƒ½è½¬ä¸ºintå‹
def accuracy(y_hat, y):
    return np.mean((tf.argmax(y_hat, axis=1) == y))
```











`ts.shape` è¿”å›ä¸€ä¸ªè¡¨ç¤ºTsçš„å½¢çŠ¶çš„tupleã€‚

`len(x)` è¿”å›Tsçš„ç¬¬ä¸€ä¸ªç»´åº¦ä¸­çš„å…ƒç´ çš„æ€»æ•°ã€‚

å¦‚ len( tf.ones((3,4)) ) ä¸º3.



`tf.gather(params,indices,validate_indices=None,name=None,axis=0)`    (cv)æ ¹æ®ç´¢å¼•è·å–æ•°ç»„ç‰¹å®šä¸‹æ ‡å…ƒç´ çš„ä»»åŠ¡

ä¸»è¦å‚æ•°ï¼šparamsï¼šè¢«ç´¢å¼•çš„å¼ é‡ï¼Œindicesï¼šä¸€ç»´ç´¢å¼•å¼ é‡ï¼Œnameï¼šè¿”å›å¼ é‡åç§°              

  è¿”å›å€¼ï¼šé€šè¿‡indicesè·å–paramsä¸‹æ ‡çš„å¼ é‡ã€‚

ä¾‹å­ï¼š

```python
import tensorflow as tf
tensor_a = tf.Variable([[1,2,3],[4,5,6],[7,8,9]])
tensor_b = tf.Variable([1,2,0],dtype=tf.int32)
tensor_c = tf.Variable([0,0],dtype=tf.int32)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(tf.gather(tensor_a,tensor_b)))
    print(sess.run(tf.gather(tensor_a,tensor_c)))
#ä¸Šä¸ªä¾‹å­tf.gather(tensor_a,tensor_b) çš„å€¼ä¸º[[4,5,6],[7,8,9],
#[1,2,3]],tf.gather(tensor_a,tensor_b) çš„å€¼ä¸º[[1,2,3],1,2,3]]


```

> 2.tf.gather_nd(params,indices,name=None)
>
> åŠŸèƒ½å’Œå‚æ•°ä¸tf.gatherç±»ä¼¼ï¼Œä¸åŒä¹‹å¤„åœ¨äºtf.gather_ndæ”¯æŒå¤šç»´åº¦ç´¢å¼•ã€‚
>
> ä¾‹å­ï¼š
>
> import tensorflow as tf
> tensor_a = tf.Variable([[1,2,3],[4,5,6],[7,8,9]])
> tensor_b = tf.Variable([[1,0],[1,1],[1,2]],dtype=tf.int32)
> tensor_c = tf.Variable([[0,2],[2,0]],dtype=tf.int32)
> with tf.Session() as sess:
> â€‹    sess.run(tf.global_variables_initializer())
> â€‹    print(sess.run(tf.gather_nd(tensor_a,tensor_b)))
> â€‹    print(sess.run(tf.gather_nd(tensor_a,tensor_c)))
> tf.gather_nd(tensor_a,tensor_b)å€¼ä¸º[4,5,6],tf.gather_nd(tensor_a,tensor_c)çš„å€¼ä¸º[3,7].
>
> å¯¹äºtensor_a,ä¸‹æ ‡[1,0]çš„å…ƒç´ ä¸º4,ä¸‹æ ‡ä¸º[1,1]çš„å…ƒç´ ä¸º5,ä¸‹æ ‡ä¸º[1,2]çš„å…ƒç´ ä¸º6,ç´¢å¼•[1,0],[1,1],[1,2]]çš„è¿”å›å€¼ä¸º[4,5,6],åŒæ ·çš„ï¼Œç´¢å¼•[[0,2],[2,0]]çš„è¿”å›å€¼ä¸º[3,7].
>
> https://www.tensorflow.org/api_docs/python/tf/gather_nd
>
> 3.tf.batch_gather(params,indices,name=None)
>
> æ”¯æŒå¯¹å¼ é‡çš„æ‰¹é‡ç´¢å¼•ï¼Œå„å‚æ•°æ„ä¹‰è§ï¼ˆ1ï¼‰ä¸­æè¿°ã€‚æ³¨æ„å› ä¸ºæ˜¯æ‰¹å¤„ç†ï¼Œæ‰€ä»¥indicesè¦æœ‰å’Œparamsç›¸åŒçš„ç¬¬0ä¸ªç»´åº¦ã€‚
>
> ä¾‹å­ï¼š
>
> import tensorflow as tf
> tensor_a = tf.Variable([[1,2,3],[4,5,6],[7,8,9]])
> tensor_b = tf.Variable([[0],[1],[2]],dtype=tf.int32)
> tensor_c = tf.Variable([[0],[0],[0]],dtype=tf.int32)
> with tf.Session() as sess:
> â€‹    sess.run(tf.global_variables_initializer())
> â€‹    print(sess.run(tf.batch_gather(tensor_a,tensor_b)))
> â€‹    print(sess.run(tf.batch_gather(tensor_a,tensor_c)))
> tf.gather_nd(tensor_a,tensor_b)å€¼ä¸º[1,5,9],tf.gather_nd(tensor_a,tensor_c)çš„å€¼ä¸º[1,4,7].
>
> tensor_açš„ä¸‰ä¸ªå…ƒç´ [1,2,3],[4,5,6],[7,8,9]åˆ†åˆ«å¯¹åº”ç´¢å¼•å…ƒç´ çš„ç¬¬ä¸€ï¼Œç¬¬äºŒå’Œç¬¬ä¸‰ä¸ªå€¼ã€‚[1,2,3]çš„ç¬¬0ä¸ªå…ƒç´ ä¸º1,[4,5,6]çš„ç¬¬1ä¸ªå…ƒç´ ä¸º5,[7,8,9]çš„ç¬¬2ä¸ªå…ƒç´ ä¸º9,æ‰€ä»¥ç´¢å¼•[[0],[1],[2]]çš„è¿”å›å€¼ä¸º[1,5,9],åŒæ ·åœ°ï¼Œç´¢å¼•[[0],[0],[0]]çš„è¿”å›å€¼ä¸º[1,4,7].
>
> https://www.tensorflow.org/api_docs/python/tf/batch_gather
>
>  
>
>     åœ¨æ·±åº¦å­¦ä¹ çš„æ¨¡å‹è®­ç»ƒä¸­ï¼Œæœ‰æ—¶å€™éœ€è¦å¯¹ä¸€ä¸ªbatchçš„æ•°æ®è¿›è¡Œç±»ä¼¼äºtf.gather_ndçš„æ“ä½œï¼Œä½†tensorflowä¸­å¹¶æ²¡æœ‰tf.batch_gather_ndä¹‹ç±»çš„æ“ä½œï¼Œæ­¤æ—¶éœ€è¦tf.map_fnå’Œtf.gather_ndç»“åˆæ¥å®ç°ä¸Šè¿°æ“ä½œã€‚
>
> 

## è¿ç®—

ç›´æ¥ä¸¾ä¾‹

```python
#è¿ç®—ç¬¦ï¼ˆoperator) 
#æŒ‰å…ƒç´ åŠ 
X + Y

#æŒ‰å…ƒç´ ä¹˜æ³•ï¼š
X * Y

#æŒ‰å…ƒç´ é™¤æ³•ï¼š
X / Y

#æŒ‰å…ƒç´ åšæŒ‡æ•°è¿ç®—ï¼š
Y = tf.cast(Y, tf.float32)
tf.exp(Y)

#matmulå‡½æ•°åšçŸ©é˜µä¹˜æ³•ã€‚ä¸‹é¢å°†Xä¸Yçš„è½¬ç½®åšçŸ©é˜µä¹˜æ³•ã€‚ç”±äºXæ˜¯3è¡Œ4åˆ—çš„çŸ©é˜µï¼ŒYè½¬ç½®ä¸º4è¡Œ3åˆ—çš„çŸ©é˜µï¼Œå› æ­¤ä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜å¾—åˆ°3è¡Œ3åˆ—çš„çŸ©é˜µã€‚
Y = tf.cast(Y, tf.int32)
tf.matmul(X, tf.transpose(Y))

#concatenateï¼‰ã€‚ä¸‹é¢åˆ†åˆ«åœ¨è¡Œä¸Šï¼ˆç»´åº¦0ï¼Œå³å½¢çŠ¶ä¸­çš„æœ€å·¦è¾¹å…ƒç´ ï¼‰å’Œåˆ—ä¸Šï¼ˆç»´åº¦1ï¼Œå³å½¢çŠ¶ä¸­å·¦èµ·ç¬¬äºŒä¸ªå…ƒç´ ï¼‰è¿ç»“ä¸¤ä¸ªçŸ©é˜µ
tf.concat([X,Y],axis = 0), tf.concat([X,Y],axis = 1)

#ä½¿ç”¨æ¡ä»¶åˆ¤æ–­å¼å¯ä»¥å¾—åˆ°å…ƒç´ ä¸º0æˆ–1çš„æ–°çš„tensorã€‚ä»¥X == Yä¸ºä¾‹ï¼Œå¦‚æœXå’ŒYåœ¨ç›¸åŒä½ç½®çš„æ¡ä»¶åˆ¤æ–­ä¸ºçœŸï¼ˆå€¼ç›¸ç­‰ï¼‰ï¼Œé‚£ä¹ˆæ–°çš„tensoråœ¨ç›¸åŒä½ç½®çš„å€¼ä¸º1ï¼›åä¹‹ä¸º0ã€‚

tf.equal(X,Y)
#{<tf.Tensor: shape=(3, 4), dtype=bool, numpy=
array([[False,  True, False,  True],
       [False, False, False, False],
       [False, False, False, False]])>
       }#
       
#å¯¹tensorä¸­çš„æ‰€æœ‰å…ƒç´ æ±‚å’Œå¾—åˆ°åªæœ‰ä¸€ä¸ªå…ƒç´ çš„tensorã€‚
## æ±‚å’Œ
tf.reduce_sum(X) #
#<tf.Tensor: shape=(), dtype=int32, numpy=66>
#æ³¨æ„ ä¸Šé¢å’Œä¸‹é¢çš„shape æ˜¯ç©ºçš„

#èŒƒæ•°
èŒƒæ•°(norm)æ˜¯æ•°å­¦ä¸­çš„ä¸€ç§åŸºæœ¬æ¦‚å¿µã€‚åœ¨æ³›å‡½åˆ†æä¸­ï¼Œå®ƒå®šä¹‰åœ¨èµ‹èŒƒçº¿æ€§ç©ºé—´ä¸­ï¼Œå¹¶æ»¡è¶³ä¸€å®šçš„æ¡ä»¶ï¼Œå³â‘ éè´Ÿæ€§ï¼›â‘¡é½æ¬¡æ€§ï¼›â‘¢ä¸‰è§’ä¸ç­‰å¼ã€‚å®ƒå¸¸å¸¸è¢«ç”¨æ¥åº¦é‡æŸä¸ªå‘é‡ç©ºé—´ï¼ˆæˆ–çŸ©é˜µï¼‰ä¸­çš„æ¯ä¸ªå‘é‡çš„é•¿åº¦æˆ–å¤§å°ã€‚

#tf.norm(X) default order='Euclidean' Euclidean


tf.norm(X)
X = tf.cast(X, tf.float32)
```

>The Frobenius norm, sometimes also called the Euclidean norm (a term unfortunately also used for the vector L^2-norm), is matrix norm of an mÃ—n matrix A defined as the square root of the sum of the absolute squares of its elements,
>
>$$
>||A||_F=sqrt(\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2)
>$$
>



## å¹¿æ’­

å½“å¯¹ä¸¤ä¸ªå½¢çŠ¶ä¸åŒçš„tensoræŒ‰å…ƒç´ è¿ç®—æ—¶ï¼Œå¯èƒ½ä¼šè§¦å‘å¹¿æ’­ï¼ˆbroadcastingï¼‰æœºåˆ¶ï¼šå…ˆé€‚å½“å¤åˆ¶å…ƒç´ ä½¿è¿™ä¸¤ä¸ªtensorå½¢çŠ¶ç›¸åŒåå†æŒ‰å…ƒç´ è¿ç®—ã€‚

å®šä¹‰ä¸¤ä¸ªtensor

```python
>>>A = tf.reshape(tf.constant(range(3)), (3,1))
>>>B = tf.reshape(tf.constant(range(2)), (1,2)
(<tf.Tensor: shape=(3, 1), dtype=int32, numpy=
 array([[0],
        [1],
        [2]])>,
 <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]])>
 
```

ç”±äºAå’ŒBåˆ†åˆ«æ˜¯3è¡Œ1åˆ—å’Œ1è¡Œ2åˆ—çš„çŸ©é˜µï¼Œå¦‚æœè¦è®¡ç®—A + Bï¼Œé‚£ä¹ˆAä¸­ç¬¬ä¸€åˆ—çš„3ä¸ªå…ƒç´ è¢«å¹¿æ’­ï¼ˆå¤åˆ¶ï¼‰åˆ°äº†ç¬¬äºŒåˆ—ï¼Œè€ŒBä¸­ç¬¬ä¸€è¡Œçš„2ä¸ªå…ƒç´ è¢«å¹¿æ’­ï¼ˆå¤åˆ¶ï¼‰åˆ°äº†ç¬¬äºŒè¡Œå’Œç¬¬ä¸‰è¡Œã€‚å¦‚æ­¤ï¼Œå°±å¯ä»¥å¯¹2ä¸ª3è¡Œ2åˆ—çš„çŸ©é˜µæŒ‰å…ƒç´ ç›¸åŠ ã€‚

```python
>>> A+B
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[0, 1],
       [1, 2],
       [2, 3]])>
```

## ç´¢å¼•

ç´¢å¼•é‡‡å–ä¸Numpyçš„Narrayç±»ä¼¼çš„å½¢å¼ï¼Œ

å¦‚ä¸€ä¸ª4*3çš„Ts

```
>>>X[1:3]
tf.Tensor(
[[ 4.  5.  6.  7.]
 [ 8.  9. 10. 11.]], shape=(2, 4), dtype=float32)
>>>X[1:3, 1:3]
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 5.,  6.],
       [ 9., 10.]], dtype=float32)> 
```

**èµ‹å€¼**

assignæ—¢å¯ä»¥å¯¹å•ä¸ªå…ƒç´ è¿›è¡Œèµ‹å€¼ï¼Œä¹Ÿå¯ä»¥å¯¹æˆªå–çš„ä¸€éƒ¨åˆ†å…ƒç´ è¿›è¡Œèµ‹å€¼ã€‚

```python
>>>X = tf.Variable(X) # åˆ›å»ºä¸€ä¸ªå˜é‡
>>>X[1,2].assign(9)
<tf.Variable 'UnreadVariable' shape=(3, 4) dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  9.,  7.],
       [ 8.,  9., 10., 11.]], dtype=float32)>

# ä¸è®ºæ˜¯æˆªå–ä¸€éƒ¨åˆ†çš„tfå˜é‡ï¼Œè¿˜æ˜¯ç”Ÿæˆä¸€ä¸ªæ–°çš„Conæˆ–è€…Varï¼Œä»–ä»¬éƒ½tfçš„Tensor
>>>X[1:2,:].assign(tf.ones(X[1:2,:].shape, dtype = tf.float32)*12)
#ä¹Ÿå¯ä»¥è¿™ä¹ˆèµ‹å€¼
>>>c.assign(a + b)

```





## èŠ‚çº¦å†…å­˜

Tsçš„è¿ç®—ç»“æœéƒ½ä¼šé‡æ–°å¼€ä¸€ä¸ªå†…å­˜æ¥ä¿å­˜ç»“æœï¼Œå¦‚æœæƒ³èŠ‚çº¦å†…å­˜ï¼Œå¯ä»¥ä½¿ç”¨`assign_{è¿ç®—ç¬¦å…¨å}`å‡½æ•°æ¥å‡å°‘å†…å­˜ã€‚

```python
x.assing_sub(Y) 
X - Y # ä¸¤è€…ç­‰ä»·
```

## è‡ªåŠ¨æ±‚æ¢¯åº¦

Tfä½¿ç”¨GradientTape() æ¥ç›‘è§†è‡³å°‘ä¸€ä¸ªå˜é‡ï¼Œæ¥è®¡ç®—å¯¼æ•°ã€‚

```python
>>>x = tf.reshape(tf.Variable(range(4), dtype=tf.float32),(4,1))
<tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[0.],
       [1.],
       [2.],
       [3.]], dtype=float32)>

#å‡½æ•°ğ‘¦=2ğ‘¥^âŠ¤ğ‘¥å…³äºxçš„æ¢¯åº¦åº”ä¸º 4ğ‘¥ ã€‚ç°åœ¨æˆ‘ä»¬æ¥éªŒè¯ä¸€ä¸‹æ±‚å‡ºæ¥çš„æ¢¯åº¦æ˜¯æ­£ç¡®çš„
>>>with tf.GradientTape() as t:
        t.watch(x)
        y = 2 * tf.matmul(tf.transpose(x), x)
    
   dy_dx = t.gradient(y, x)
>>>dy_dx
<tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[ 0.],
       [ 4.],
       [ 8.],
       [12.]], dtype=float32)>


```

[2.3.3 å¯¹Pythonæ§åˆ¶æµæ±‚æ¢¯åº¦](https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter02_prerequisite/2.3_autograd?id=_233-å¯¹pythonæ§åˆ¶æµæ±‚æ¢¯åº¦)è¿™é‡Œè®²è¿°äº†åœ¨æ§åˆ¶æµä¸­ä¹Ÿå¯ä»¥è¿›è¡Œæ¢¯åº¦æ±‚å¯¼ã€‚



## æ•°æ®åˆ’åˆ†

```python
batch_size = 10
# å°†è®­ç»ƒæ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ç»„åˆ
dataset = tfdata.Dataset.from_tensor_slices((features, labels))
# éšæœºè¯»å–å°æ‰¹é‡
dataset = dataset.shuffle(buffer_size=num_examples)
dataset = dataset.batch(batch_size)
data_iter = iter(dataset) # å¯ä»¥çœ‹åˆ°è¿™é‡Œåˆ›å»ºä¸¤ä¸ªä¸€ä¸ªæ–°çš„datasetçš„iter
```



## å®šä¹‰æ¨¡å‹





### Keras.Sequential()

`Tensorflow 2.0`æ¨èä½¿ç”¨`Keras`å®šä¹‰ç½‘ç»œï¼Œæ•…ä½¿ç”¨`Keras`å®šä¹‰ç½‘ç»œ æˆ‘ä»¬å…ˆå®šä¹‰ä¸€ä¸ªæ¨¡å‹å˜é‡`model`ï¼Œå®ƒæ˜¯ä¸€ä¸ª`Sequential`å®ä¾‹ã€‚ åœ¨`Keras`ä¸­ï¼Œ`Sequential`å®ä¾‹å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªä¸²è”å„ä¸ªå±‚çš„å®¹å™¨ã€‚

åœ¨æ„é€ æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬åœ¨è¯¥å®¹å™¨ä¸­ä¾æ¬¡æ·»åŠ å±‚ã€‚ å½“ç»™å®šè¾“å…¥æ•°æ®æ—¶ï¼Œå®¹å™¨ä¸­çš„æ¯ä¸€å±‚å°†ä¾æ¬¡æ¨æ–­ä¸‹ä¸€å±‚çš„è¾“å…¥å°ºå¯¸ã€‚ é‡è¦çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨`Keras`ä¸­æˆ‘ä»¬**æ— é¡»æŒ‡å®šæ¯ä¸€å±‚è¾“å…¥çš„å½¢çŠ¶**ã€‚ çº¿æ€§å›å½’ï¼Œè¾“å…¥å±‚ä¸è¾“å‡ºå±‚ç­‰æ•ˆä¸ºä¸€å±‚å…¨è¿æ¥å±‚`keras.layers.Dense()`ã€‚

`Keras` ä¸­åˆå§‹åŒ–å‚æ•°ç”± `kernel_initializer` å’Œ `bias_initializer` é€‰é¡¹åˆ†åˆ«è®¾ç½®æƒé‡å’Œåç½®çš„åˆå§‹åŒ–æ–¹å¼ã€‚æˆ‘ä»¬ä» `tensorflow` å¯¼å…¥ `initializers` æ¨¡å—ï¼ŒæŒ‡å®šæƒé‡å‚æ•°æ¯ä¸ªå…ƒç´ å°†åœ¨åˆå§‹åŒ–æ—¶éšæœºé‡‡æ ·äºå‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒã€‚åå·®å‚æ•°é»˜è®¤ä¼šåˆå§‹åŒ–ä¸ºé›¶ã€‚`RandomNormal(stddev=0.01)`æŒ‡å®šæƒé‡å‚æ•°æ¯ä¸ªå…ƒç´ å°†åœ¨åˆå§‹åŒ–æ—¶éšæœºé‡‡æ ·äºå‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒã€‚åå·®å‚æ•°é»˜è®¤ä¼šåˆå§‹åŒ–ä¸ºé›¶ã€‚

```python
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow import initializers as init
model = keras.Sequential()
model.add(layers.Dense(1, kernel_initializer=init.RandomNormal(stddev=0.01)))
```



å¯¹äºä½¿ç”¨`Sequential`ç±»æ„é€ çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡weightså±æ€§æ¥è®¿é—®ç½‘ç»œä»»ä¸€å±‚çš„æƒé‡ã€‚å›å¿†ä¸€ä¸‹ä¸Šä¸€èŠ‚ä¸­æåˆ°çš„`Sequential`ç±»ä¸`tf.keras.Model`ç±»çš„ç»§æ‰¿å…³ç³»ã€‚å¯¹äº`Sequential`å®ä¾‹ä¸­å«æ¨¡å‹å‚æ•°çš„å±‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡`tf.keras.Model`ç±»çš„`weights`å±æ€§æ¥è®¿é—®è¯¥å±‚åŒ…å«çš„æ‰€æœ‰å‚æ•°ã€‚ä¸‹é¢ï¼Œè®¿é—®å¤šå±‚æ„ŸçŸ¥æœº`net`ä¸­éšè—å±‚çš„æ‰€æœ‰å‚æ•°ã€‚ç´¢å¼•0è¡¨ç¤ºéšè—å±‚ä¸º`Sequential`å®ä¾‹æœ€å…ˆæ·»åŠ çš„å±‚ã€‚

å®é™…ä¸Šï¼Œ`tf.keras.Model`çš„å­ç±»éƒ½å¯ä»¥é€šè¿‡`weughts`å‚æ•°æ¥è®¿é—®è¯¥æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚æ¯”å¦‚

```python
net = tf.keras.models.Sequential()
net.add(tf.keras.layers.Flatten())
net.add(tf.keras.layers.Dense(256,activation=tf.nn.relu))
net.add(tf.keras.layers.Dense(10))

X = tf.random.uniform((2,20))
Y = net(X)

#å¯è®¿é—® Sequentialæ¨¡å‹ä¸­æœ€å…ˆåŠ å…¥çš„layerçš„å‚æ•°
net.weights[0], type(net.weights[0])

```





### build model from block 

å¯ä»¥ä»`tf.keras`æ¨¡å—ä¸­æä¾›çš„ä¸€ä¸ªæ¨¡å‹æ„é€ ç±»`tf.keras.Model`ç»§æ‰¿æˆ‘ä»¬æ‰€éœ€è¦çš„ç±»ã€‚

å¦‚

```python
class MLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        #ï¼Ÿï¼Ÿ ä¸ºå•¥è¿™é‡Œå¯ä»¥æ²¡å‚æ•°
        self.flatten = tf.keras.layers.Flatten()    # Flattenå±‚å°†é™¤ç¬¬ä¸€ç»´ï¼ˆbatch_sizeï¼‰ä»¥å¤–çš„ç»´åº¦å±•å¹³
        self.dense1 = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(units=10)

    def call(self, inputs):         
        x = self.flatten(inputs)   
        x = self.dense1(x)    
        output = self.dense2(x)     
        return output
```

ä»¥ä¸Šçš„`MLP`ç±»ä¸­**æ— é¡»å®šä¹‰åå‘ä¼ æ’­å‡½æ•°**ã€‚ç³»ç»Ÿå°†é€šè¿‡è‡ªåŠ¨æ±‚æ¢¯åº¦è€Œè‡ªåŠ¨ç”Ÿæˆåå‘ä¼ æ’­æ‰€éœ€çš„`backward`å‡½æ•°ã€‚

æˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–`MLP`ç±»å¾—åˆ°æ¨¡å‹å˜é‡`net`ã€‚ä¸‹é¢çš„ä»£ç åˆå§‹åŒ–`net`å¹¶ä¼ å…¥è¾“å…¥æ•°æ®`X`åšä¸€æ¬¡å‰å‘è®¡ç®—ã€‚å…¶ä¸­ï¼Œ`net(X)`å°†è°ƒç”¨`MLP`ç±»å®šä¹‰çš„`call`å‡½æ•°æ¥å®Œæˆå‰å‘è®¡ç®—ã€‚

```python
X = tf.random.uniform((2,20))
net = MLP()
net(X)
```





### build complex modelï¼ˆè¿™ä¸€éƒ¨åˆ†è¿˜ä¸å¤ªæ˜ç™½@@ï¼Œå ç½—æ±‰ï¼‰



è™½ç„¶`Sequential`ç±»å¯ä»¥ä½¿æ¨¡å‹æ„é€ æ›´åŠ ç®€å•ï¼Œä¸”ä¸éœ€è¦å®šä¹‰`call`å‡½æ•°ï¼Œä½†ç›´æ¥ç»§æ‰¿`tf.keras.Model`ç±»å¯ä»¥æå¤§åœ°æ‹“å±•æ¨¡å‹æ„é€ çš„çµæ´»æ€§ã€‚ä¸‹é¢æˆ‘ä»¬æ„é€ ä¸€ä¸ªç¨å¾®å¤æ‚ç‚¹çš„ç½‘ç»œ`FancyMLP`ã€‚åœ¨è¿™ä¸ªç½‘ç»œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡`constant`å‡½æ•°åˆ›å»ºè®­ç»ƒä¸­ä¸è¢«è¿­ä»£çš„å‚æ•°ï¼Œå³å¸¸æ•°å‚æ•°ã€‚åœ¨å‰å‘è®¡ç®—ä¸­ï¼Œé™¤äº†ä½¿ç”¨åˆ›å»ºçš„å¸¸æ•°å‚æ•°å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨`tensor`çš„å‡½æ•°å’ŒPythonçš„æ§åˆ¶æµï¼Œå¹¶å¤šæ¬¡è°ƒç”¨ç›¸åŒçš„å±‚ã€‚

```python
class FancyMLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.flatten = tf.keras.layers.Flatten()
        self.rand_weight = tf.constant(
            tf.random.uniform((20,20)))
        self.dense = tf.keras.layers.Dense(units=20, activation=tf.nn.relu)

    def call(self, inputs):         
        x = self.flatten(inputs)   
        x = tf.nn.relu(tf.matmul(x, self.rand_weight) + 1)
        x = self.dense(x)    
        while tf.norm(x) > 1:
            x /= 2
        if tf.norm(x) < 0.8:
            x *= 10
        return tf.reduce_sum(x)
```

åœ¨è¿™ä¸ª`FancyMLP`æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¸¸æ•°æƒé‡`rand_weight`ï¼ˆæ³¨æ„å®ƒä¸æ˜¯æ¨¡å‹å‚æ•°ï¼‰ã€åšäº†çŸ©é˜µä¹˜æ³•æ“ä½œï¼ˆ`tf.matmul`ï¼‰å¹¶é‡å¤ä½¿ç”¨äº†ç›¸åŒçš„`Dense`å±‚ã€‚ä¸‹é¢æˆ‘ä»¬æ¥æµ‹è¯•è¯¥æ¨¡å‹çš„éšæœºåˆå§‹åŒ–å’Œå‰å‘è®¡ç®—ã€‚

```python
net = FancyMLP()
net(X)
<tf.Tensor: id=220, shape=(), dtype=float32, numpy=24.381481>
```

å› ä¸º`FancyMLP`å’Œ`Sequential`ç±»éƒ½æ˜¯`tf.keras.Model`ç±»çš„å­ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åµŒå¥—è°ƒç”¨å®ƒä»¬ã€‚

```python
class NestMLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.net = tf.keras.Sequential()
        self.net.add(tf.keras.layers.Flatten())
        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))
        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))
        self.dense = tf.keras.layers.Dense(units=16, activation=tf.nn.relu)


    def call(self, inputs):         
        return self.dense(self.net(inputs))

net = tf.keras.Sequential()
net.add(NestMLP())
net.add(tf.keras.layers.Dense(20))
net.add(FancyMLP())

net(X)
<tf.Tensor: id=403, shape=(), dtype=float32, numpy=3.2303767>
```





### è‡ªå®šä¹‰layer



æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªé­…åŠ›åœ¨äºç¥ç»ç½‘ç»œä¸­å„å¼å„æ ·çš„å±‚ï¼Œä¾‹å¦‚å…¨è¿æ¥å±‚å’Œåé¢ç« èŠ‚ä¸­å°†è¦ä»‹ç»çš„å·ç§¯å±‚ã€æ± åŒ–å±‚ä¸å¾ªç¯å±‚ã€‚è™½ç„¶tf.kerasæä¾›äº†å¤§é‡å¸¸ç”¨çš„å±‚ï¼Œä½†æœ‰æ—¶å€™æˆ‘ä»¬ä¾ç„¶å¸Œæœ›è‡ªå®šä¹‰å±‚ã€‚æœ¬èŠ‚å°†ä»‹ç»å¦‚ä½•è‡ªå®šä¹‰ä¸€ä¸ªå±‚ï¼Œä»è€Œå¯ä»¥è¢«é‡å¤è°ƒç”¨ã€‚

```python
X = tf.random.uniform((2,20))
```

######  custom layer without parameters

æˆ‘ä»¬å…ˆä»‹ç»å¦‚ä½•å®šä¹‰ä¸€ä¸ªä¸å«æ¨¡å‹å‚æ•°çš„è‡ªå®šä¹‰å±‚ã€‚äº‹å®ä¸Šï¼Œè¿™å’Œ[â€œæ¨¡å‹æ„é€ â€]ä¸€èŠ‚ä¸­ä»‹ç»çš„ä½¿ç”¨`tf.keras.Model`ç±»æ„é€ æ¨¡å‹ç±»ä¼¼ã€‚ä¸‹é¢çš„`CenteredLayer`ç±»é€šè¿‡ç»§æ‰¿`tf.keras.layers.Layer`ç±»è‡ªå®šä¹‰äº†ä¸€ä¸ªå°†è¾“å…¥å‡æ‰å‡å€¼åè¾“å‡ºçš„å±‚ï¼Œå¹¶å°†å±‚çš„è®¡ç®—å®šä¹‰åœ¨äº†`call`å‡½æ•°é‡Œã€‚è¿™ä¸ªå±‚é‡Œä¸å«æ¨¡å‹å‚æ•°ã€‚

```python
class CenteredLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

    def call(self, inputs):
        return inputs - tf.reduce_mean(inputs)
```

æˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–è¿™ä¸ªå±‚ï¼Œç„¶ååšå‰å‘è®¡ç®—ã€‚

```python
layer = CenteredLayer()
layer(np.array([1,2,3,4,5]))
<tf.Tensor: id=11, shape=(5,), dtype=int32, numpy=array([-2, -1,  0,  1,  2])>
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨å®ƒæ¥æ„é€ æ›´å¤æ‚çš„æ¨¡å‹ã€‚

```python
net = tf.keras.models.Sequential()
net.add(tf.keras.layers.Flatten())
net.add(tf.keras.layers.Dense(20))
net.add(CenteredLayer())

Y = net(X)
Y
<tf.Tensor: id=42, shape=(2, 20), dtype=float32, numpy=
array([[-0.2791378 , -0.80257636, -0.8498672 , -0.8917849 , -0.43128002,
         0.2557137 , -0.51745236,  0.31894356,  0.03016172,  0.5299317 ,
        -0.094203  , -0.3885942 ,  0.6737736 ,  0.5981153 ,  0.30068082,
         0.42632163,  0.3067779 ,  0.07029241,  0.0343143 ,  0.41021633],
       [ 0.0257766 , -0.4703896 , -0.9074424 , -1.2818251 ,  0.17860745,
         0.11847494, -0.14939149,  0.20248316, -0.140678  ,  0.6033463 ,
         0.13899392, -0.08732668,  0.08497022,  0.8094018 ,  0.20579913,
         0.40613335,  0.2509889 ,  0.34718364, -0.6298219 ,  0.59436864]],
      dtype=float32)>
```

ä¸‹é¢æ‰“å°è‡ªå®šä¹‰å±‚å„ä¸ªè¾“å‡ºçš„å‡å€¼ã€‚å› ä¸ºå‡å€¼æ˜¯æµ®ç‚¹æ•°ï¼Œæ‰€ä»¥å®ƒçš„å€¼æ˜¯ä¸€ä¸ªå¾ˆæ¥è¿‘0çš„æ•°ã€‚

```python
tf.reduce_mean(Y)
<tf.Tensor: id=44, shape=(), dtype=float32, numpy=-2.9802323e-09>
```

##### 4.4.2 custom layer with parameters

æˆ‘ä»¬è¿˜å¯ä»¥è‡ªå®šä¹‰å«æ¨¡å‹å‚æ•°çš„è‡ªå®šä¹‰å±‚ã€‚å…¶ä¸­çš„æ¨¡å‹å‚æ•°å¯ä»¥é€šè¿‡è®­ç»ƒå­¦å‡ºã€‚

```python
class myDense(tf.keras.layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.units = units
	#ä½¿ç”¨buildå»ºç«‹å‚æ•°
    def build(self, input_shape):     # è¿™é‡Œ input_shape æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œcall()æ—¶å‚æ•°inputsçš„å½¢çŠ¶
        self.w = self.add_weight(name='w',
            shape=[input_shape[-1], self.units], initializer=tf.random_normal_initializer())
        self.b = self.add_weight(name='b',
            shape=[self.units], initializer=tf.zeros_initializer())

    def call(self, inputs):
        y_pred = tf.matmul(inputs, self.w) + self.b
        return y_pred
```

ä¸‹é¢ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–`MyDense`ç±»å¹¶è®¿é—®å®ƒçš„æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨è‡ªå®šä¹‰å±‚åšå‰å‘è®¡ç®—ã€‚

```python
dense = myDense(3)
dense(X)
dense.get_weights()
[array([[ 0.05307531, -0.01968029,  0.00317079],
        [-0.03745286, -0.0031012 , -0.0925727 ],
        [ 0.00653961, -0.0849395 , -0.00591413],
        [-0.03926834,  0.03737333, -0.08176559],
        [-0.02961348,  0.00735149, -0.04053285],
        [-0.0769348 , -0.01365675,  0.04430145],
        [ 0.05790468,  0.06002709,  0.00588025],
        [ 0.00912714, -0.04544574, -0.08150417],
        [ 0.01794734, -0.06478786, -0.0466853 ],
        [ 0.0007794 ,  0.07972597,  0.01827623],
        [ 0.04688237,  0.040658  ,  0.04173873],
        [ 0.07974287, -0.01226464,  0.03872328],
        [ 0.023996  , -0.044014  ,  0.01851312],
        [-0.04491149,  0.00450119,  0.03688556],
        [ 0.01733875, -0.01641337,  0.06909126],
        [-0.07539   , -0.0878872 ,  0.0091918 ],
        [-0.00092481, -0.06399333,  0.00150875],
        [-0.01826238, -0.06126164, -0.05938709],
        [ 0.04794892,  0.03742057, -0.0018529 ],
        [ 0.03086024,  0.00513093, -0.04271856]], dtype=float32),
 array([0., 0., 0.], dtype=float32)]
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰å±‚æ„é€ æ¨¡å‹ã€‚

```python
net = tf.keras.models.Sequential()
net.add(myDense(8))
net.add(myDense(1))

net(X)
<tf.Tensor: id=121, shape=(2, 1), dtype=float32, numpy=
array([[-0.00446665],
       [-0.0158301 ]], dtype=float32)>
```





## Layer

`tf.keras.layers.Embedding`

Embedding layerçš„ä½œç”¨ä¸»è¦åœ¨äºå­¦ä¹ è¯è¯­çš„distributed representationå¹¶å°†æå…¶ç¨€ç–çš„one-hotç¼–ç çš„è¯è¯­è¿›è¡Œé™ç»´ã€‚

## éšæœºåˆå§‹åŒ–æ¨¡å‹å‚æ•°





ä¸ºäº†é¿å…ç¥ç»ç½‘ç»œä¼ æ’­çš„å„ä¸ªèŠ‚ç‚¹çš„å˜åŒ–å®Œå…¨ä¸€æ ·ï¼Œä»¥åŠå‚æ•°çˆ†ç‚¸å’Œè¡°å‡é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œå‚æ•°åˆå§‹åŒ–



###  Tensorflow2.0çš„é»˜è®¤éšæœºåˆå§‹åŒ–

éšæœºåˆå§‹åŒ–æ¨¡å‹å‚æ•°çš„æ–¹æ³•æœ‰å¾ˆå¤šã€‚åœ¨3.3èŠ‚ï¼ˆçº¿æ€§å›å½’çš„ç®€æ´å®ç°ï¼‰ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`kernel_initializer=init.RandomNormal(stddev=0.01)`ä½¿æ¨¡å‹`model`çš„æƒé‡å‚æ•°é‡‡ç”¨æ­£æ€åˆ†å¸ƒçš„éšæœºåˆå§‹åŒ–æ–¹å¼ã€‚ä¸è¿‡ï¼ŒTensorflowä¸­`initializers`çš„æ¨¡å—å‚æ•°éƒ½é‡‡å–äº†è¾ƒä¸ºåˆç†çš„åˆå§‹åŒ–ç­–ç•¥ï¼ˆä¸åŒç±»å‹çš„layerå…·ä½“é‡‡æ ·çš„å“ªä¸€ç§åˆå§‹åŒ–æ–¹æ³•çš„å¯å‚è€ƒ[æºä»£ç ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers)ï¼‰ï¼Œå› æ­¤ä¸€èˆ¬ä¸ç”¨æˆ‘ä»¬è€ƒè™‘ã€‚

### Xavieréšæœºåˆå§‹åŒ–

è¿˜æœ‰ä¸€ç§æ¯”è¾ƒå¸¸ç”¨çš„éšæœºåˆå§‹åŒ–æ–¹æ³•å«ä½œXavieréšæœºåˆå§‹åŒ–ã€‚ å‡è®¾æŸå…¨è¿æ¥å±‚çš„è¾“å…¥ä¸ªæ•°ä¸º*a*ï¼Œè¾“å‡ºä¸ªæ•°ä¸º*b*ï¼ŒXavieréšæœºåˆå§‹åŒ–å°†ä½¿è¯¥å±‚ä¸­æƒé‡å‚æ•°çš„æ¯ä¸ªå…ƒç´ éƒ½éšæœºé‡‡æ ·äºå‡åŒ€åˆ†å¸ƒ

![image-20200310095151449](Keraså’ŒTensorFlowçš„æŒ‡å—.assets/image-20200310095151449.png)

å®ƒçš„è®¾è®¡ä¸»è¦è€ƒè™‘åˆ°ï¼Œæ¨¡å‹å‚æ•°åˆå§‹åŒ–åï¼Œæ¯å±‚è¾“å‡ºçš„æ–¹å·®ä¸è¯¥å—è¯¥å±‚è¾“å…¥ä¸ªæ•°å½±å“ï¼Œä¸”æ¯å±‚æ¢¯åº¦çš„æ–¹å·®ä¹Ÿä¸è¯¥å—è¯¥å±‚è¾“å‡ºä¸ªæ•°å½±å“ã€‚



## æ¨¡å‹å®šä¹‰ä¸­çš„åˆå§‹åŒ–

æˆ‘ä»¬ç»å¸¸éœ€è¦ä½¿ç”¨å…¶ä»–æ–¹æ³•æ¥åˆå§‹åŒ–æƒé‡ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æƒé‡å‚æ•°åˆå§‹åŒ–æˆå‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼Œå¹¶ä¾ç„¶å°†åå·®å‚æ•°æ¸…é›¶

```python
class Linear(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.d1 = tf.keras.layers.Dense(
            units=10,
            activation=None,
            kernel_initializer=tf.zeros_initializer(),
            bias_initializer=tf.zeros_initializer()
        )
        self.d2 = tf.keras.layers.Dense(
            units=1,
            activation=None,
            kernel_initializer=tf.ones_initializer(),
            bias_initializer=tf.ones_initializer()
        )

    def call(self, input):
        output = self.d1(input)
        output = self.d2(output)
        return output

net = Linear()
net(X)
net.get_weights()
```

```python
[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),
 array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),
 array([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.]], dtype=float32),
 array([1.], dtype=float32)]
```

## define initializer



å¯ä»¥ä½¿ç”¨`tf.keras.initializers`ç±»ä¸­çš„æ–¹æ³•å®ç°è‡ªå®šä¹‰åˆå§‹åŒ–ã€‚

```python
def my_init():
    return tf.keras.initializers.Ones()

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(64, kernel_initializer=my_init()))

Y = model(X)
model.weights[0]

```

```python

<tf.Variable 'sequential_1/dense_4/kernel:0' shape=(20, 64) dtype=float32, numpy=
array([[1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       ...,
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.]], dtype=f
```









## Dropout



æˆ‘ä»¬åªéœ€è¦åœ¨å…¨è¿æ¥å±‚åæ·»åŠ `Dropout`å±‚å¹¶æŒ‡å®šä¸¢å¼ƒæ¦‚ç‡ã€‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œ`Dropout`å±‚å°†ä»¥æŒ‡å®šçš„ä¸¢å¼ƒæ¦‚ç‡éšæœºä¸¢å¼ƒä¸Šä¸€å±‚çš„è¾“å‡ºå…ƒç´ ï¼›åœ¨æµ‹è¯•æ¨¡å‹æ—¶ï¼ˆå³`model.eval()`åï¼‰ï¼Œ`Dropout`å±‚å¹¶ä¸å‘æŒ¥ä½œç”¨ã€‚

```python
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(256,activation='relu'),
    Dropout(0.2),
    keras.layers.Dense(256,activation='relu'),
    Dropout(0.5),
    keras.layers.Dense(10,activation=tf.nn.softmax)
])
```

## æŸå¤±å‡½æ•°

`Tensoflow`åœ¨`losses`æ¨¡å—ä¸­æä¾›äº†å„ç§æŸå¤±å‡½æ•°å’Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°çš„åŸºç±»ã€‚



```python
from tensorflow import losses
#å‡æ–¹è¯¯å·®æŸå¤±
loss = losses.MeanSquaredError()
#äº¤å‰ç†µæŸå¤±
tf.losses.sparse_categorical_crossentropy(y_true,y_hat)
```





## ä¼˜åŒ–ç®—æ³•

`tensorflow.keras.optimizers` æ¨¡å—æä¾›äº†å¾ˆå¤šå¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•æ¯”å¦‚SGDã€Adamå’ŒRMSPropç­‰ã€‚

ä»¬åˆ›å»ºä¸€ä¸ªç”¨äºä¼˜åŒ–model æ‰€æœ‰å‚æ•°çš„ä¼˜åŒ–å™¨å®ä¾‹ï¼Œå¹¶æŒ‡å®šå­¦ä¹ ç‡ä¸º0.03çš„å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¸ºä¼˜åŒ–ç®—æ³•ã€‚

```python
from tensorflow.keras import optimizers
trainer = optimizers.SGD(learning_rate=0.03)
```

åŒ–å™¨å®ä¾‹æ—¶é€šè¿‡`weight_decay`å‚æ•°æ¥æŒ‡å®šæƒé‡è¡°å‡è¶…ï¼ˆweight decayï¼‰



## è®­ç»ƒæ¨¡å‹

åœ¨ä½¿ç”¨`Tensorflow`è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒç”¨`tensorflow.GradientTape`è®°å½•åŠ¨æ€å›¾æ¢¯åº¦ï¼Œæ‰§è¡Œ`tape.gradient`è·å¾—åŠ¨æ€å›¾ä¸­å„å˜é‡æ¢¯åº¦ã€‚é€šè¿‡ `model.trainable_variables` æ‰¾åˆ°éœ€è¦æ›´æ–°çš„å˜é‡ï¼Œå¹¶ç”¨ `trainer.apply_gradients` æ›´æ–°æƒé‡ï¼Œå®Œæˆä¸€æ­¥è®­ç»ƒã€‚

```python
num_epochs = 3
num_epochs = 3
for epoch in range(1, num_epochs + 1):
    for (batch, (X, y)) in enumerate(dataset):
        with tf.GradientTape() as tape:
            l = loss(model(X, training=True), y)

        grads = tape.gradient(l, model.trainable_variables)
        trainer.apply_gradients(zip(grads, model.trainable_variables))

    l = loss(model(features), labels)
    print('epoch %d, loss: %f' % (epoch, l))
```

> ä¸€äº›å…³äº`trainer.apply_gradients()`çš„å®˜æ–¹è¡¥å……
>
> Processing gradients before applying them.
>
> Calling `minimize()` takes care of both computing the gradients and applying them to the variables. If you want to process the gradients before applying them you can instead use the optimizer in three steps:
>
> 1. Compute the gradients with [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape).
> 2. Process the gradients as you wish.
> 3. Apply the processed gradients with `apply_gradients()`.
>
> å¯¹äº†`apply_gradients()`ä¹Ÿæ˜¯å¯ä»¥æ¥å—listçš„ï¼Œåº”è¯¥å†…éƒ¨æ˜¯foréå†çš„å§ï¼Ÿ

## è®­ç»ƒä¿¡æ¯

ä¸‹é¢æˆ‘ä»¬åˆ†åˆ«æ¯”è¾ƒå­¦åˆ°çš„æ¨¡å‹å‚æ•°å’ŒçœŸå®çš„æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡`model.get_weights()`æ¥è·å¾—å…¶æƒé‡ï¼ˆ`weight`ï¼‰å’Œåå·®ï¼ˆ`bias`ï¼‰ã€‚





## ä¿å­˜å’Œå­˜å‚¨

å­˜å‚¨Tsï¼Œå­—å…¸ç­‰ç­‰å˜é‡ã€‚

```python
#1
np.save('x.npy', x)
x2 = np.load('x.npy')
#2
y = tf.zeros(4)
np.save('xy.npy',[x,y])
x2, y2 = np.load('xy.npy', allow_pickle=True)
(x2, y2)
```

ä¿å­˜æ¨¡å‹

æˆ‘ä»¬å¯å·²åœ¨è®­ç»ƒå¥½æ¨¡å‹ä¹‹åï¼Œç›´æ¥æŠŠæ¨¡å‹çš„å‚æ•°ä¿å­˜ä¸‹æ¥ï¼Œç”šè‡³å¯ä»¥ç›´æ¥åº”ç”¨åˆ°ä¸€ä¸ªæ–°çš„ç›¸åŒç»“æ„çš„æ¨¡å‹ä¸Šå»ã€‚

```python
#net ä¸ºè®­ç»ƒå¥½çš„æ¨¡å‹
net.save_weights("4.5saved_model.h5")
#è½½å…¥å‚æ•°
net2(X)
net2.load_weights("4.5saved_model.h5")
```



## Numpyè”åŠ¨

### `ts.numpy()` ä»¥åŠ `ts.mean()`

The TensorFlow [`tf.math`](https://www.tensorflow.org/api_docs/python/tf/math) operations convert Python objects and NumPy arrays to [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects. The `tf.Tensor.numpy` method returns the object's value as a NumPy `ndarray`.ç±»ä¼¼çš„è¿˜æœ‰ `ts.mean()`ã€‚

`resduct_sum(x, axis=0, keepdims=True)` ä¿æŒåŸæœ‰ç»´åº¦å¹¶å¯¹åŒä¸€åˆ—ç›¸åŠ ï¼ˆæŠŠæ‰€æœ‰çš„è¡ŒåŠ ä¸€èµ·ï¼ŒåŒç†axis=1ï¼Œå°±æ˜¯æŠŠtfçš„åŒä¸€è¡Œçš„æ•°æ®åŠ ä¸€èµ·ï¼‰

> åŒæ ·çš„å®šä¹‰æ–¹å¼ï¼Œå¯ä»¥å®šä¹‰ä¸€ä¸ªsoftmax
>
> ```python
> def softmax(logits, axis=-1):
>     return tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)
> ```



## GPUé©±åŠ¨

æµ‹è¯•å¯ç”¨GPU

```python
import tensorflow as tf
import numpy as np
print(tf.__version__)

gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
cpus = tf.config.experimental.list_physical_devices(device_type='CPU')
print("å¯ç”¨çš„GPUï¼š",gpus,"\nå¯ç”¨çš„CPUï¼š", cpus)

```

æ˜¾ç¤ºå¯ç”¨è®¾å¤‡è¯¦ç»†ä¿¡æ¯

```python
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
```

æŒ‡å®šç‰¹å®šè®¾å¤‡

ä½¿ç”¨tf.device()æ¥æŒ‡å®šç‰¹å®šè®¾å¤‡(GPU/CPU)

```python
with tf.device('GPU:0'):
    a = tf.constant([1,2,3],dtype=tf.float32)
    b = tf.random.uniform((3,))
    print(tf.exp(a + b) * 2)

```









## tf.math

`tf.math.maximum(x,0)` è¿”å›æœ€å¤§å€¼





## tf.random

`tf.random.uniform(shape, minval=0, maxval=None)` è¿”å›ä»è¿ç»­åˆ†å¸ƒä¸­å¾—åˆ°çš„Tsã€‚





# SkLearning

`sklearn.model_selection.train_test_split(arrays**,shuffle)`

**arrays**:å¯ä»¥è¾“å…¥Nä¸ªarraysï¼ŒåŒæ—¶è¿”å›2\*Nä¸ªarraysã€‚

**shuffle**: boolean æ˜¯å¦shuffle

**random_state**ï¼šæŒ‡å®šéšæœºæ•°ç”Ÿæˆå™¨çš„â€œç§å­â€ï¼ˆintï¼‰ï¼Œå¦åˆ™å°±ä½¿ç”¨npçš„éšæœºæ•°ã€‚

**test_size** å’Œ **train_size**ç”¨æ¥æŒ‡å®šä¸¤ä¸ªæ•°æ®é›†çš„æ‰€å å¤§å°ï¼Œä¸€èˆ¬è®¾ç½®æµ®ç‚¹ç¬¦ï¼Œä¸¤ä¸ªå‚æ•°è®¾ç½®ä¸€ä¸ªå°±ok

```python
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.33, random_state=42)
...
>>> X_train
array([[4, 5],
       [0, 1],
       [6, 7]])
>>> y_train
[2, 0, 3]
>>> X_test
array([[2, 3],
       [8, 9]])
>>> y_test
[1, 4]
```





# ç³»ç»Ÿå‘½ä»¤

## æµ‹è¯•GPUä½¿ç”¨



```python
import tensorflow as tf
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # ä¸æ˜¾ç¤ºç­‰çº§2ä»¥ä¸‹çš„æç¤ºä¿¡æ¯

print('GPU', tf.test.is_gpu_available())

a = tf.constant(2.0)
b = tf.constant(4.0)
print(a + b)

```

## æ¸…é™¤å½“å‰Tensorç¯å¢ƒ

 åœ¨é‡å¤è¿è¡Œæ¨¡å‹çš„æ—¶å€™ï¼Œå¸¸å¸¸å‡ºç°å®šä¹‰æ¨¡å‹é”™è¯¯ï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å¯ä»¥æ¸…é™¤ä¹‹å‰å®šä¹‰çš„æ‰€æœ‰Tensorå¯¹è¯å’Œå˜é‡ã€‚

`tf.keras.backend.clear_session`

`tf.compat.v1.reset_default_graph()` ç±»ä¼¼çš„è¿™ä¸ªä¹Ÿå¯ä»¥

## ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥

nvidia-smiæ‰€åœ¨çš„ä½ç½®ä¸ºï¼š
C:\Program Files\NVIDIA Corporation\NVSMI

cmdè¿›å…¥ç›®å½•è¾“å…¥å‘½ä»¤`nvidia-smi`å³å¯ï¼š



## è·å–æœ¬åœ°å¸®åŠ©

#### dir

æŸ¥çœ‹ä¸€ä¸ªæ¨¡å—ä¸­å¯ä»¥çš„å‡½æ•°å’Œç±»

é€šå¸¸æˆ‘ä»¬å¯ä»¥å¿½ç•¥æ‰ç”±`__`å¼€å¤´å’Œç»“å°¾çš„å‡½æ•°ï¼ˆPythonçš„ç‰¹åˆ«å¯¹è±¡ï¼‰æˆ–è€…ç”±`_`å¼€å¤´çš„å‡½æ•°ï¼ˆä¸€èˆ¬ä¸ºå†…éƒ¨å‡½æ•°ï¼‰ã€‚é€šè¿‡å…¶ä½™æˆå‘˜çš„åå­—æˆ‘ä»¬å¤§è‡´çŒœæµ‹å‡ºè¿™ä¸ªramdomæ¨¡å—æä¾›äº†å„ç§éšæœºæ•°çš„ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬ä»å‡åŒ€åˆ†å¸ƒé‡‡æ ·ï¼ˆ`uniform`ï¼‰ã€ä»æ­£æ€åˆ†å¸ƒé‡‡æ ·ï¼ˆ`normal`ï¼‰ã€ä»æ³Šæ¾åˆ†å¸ƒé‡‡æ ·ï¼ˆ`poisson`ï¼‰ç­‰ã€‚

#### help

æƒ³äº†è§£æŸä¸ªå‡½æ•°æˆ–è€…ç±»çš„å…·ä½“ç”¨æ³•æ—¶ï¼Œå¯ä»¥ä½¿ç”¨`help`å‡½æ•°ã€‚è®©æˆ‘ä»¬ä»¥`ones`å‡½æ•°ä¸ºä¾‹ï¼ŒæŸ¥é˜…å®ƒçš„ç”¨æ³•ã€‚æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œå¯ä»¥é€šè¿‡[Tensorflowçš„APIæ–‡æ¡£ç‰ˆæœ¬é€‰æ‹©é¡µ](https://www.tensorflow.org/versions)ï¼Œé€‰æ‹©ä¸è‡ªå·±ç¯å¢ƒä¸­çš„ tensorflow ç‰ˆæœ¬ä¸€è‡´çš„ API ç‰ˆæœ¬è¿›è¡ŒæŸ¥è¯¢ã€‚

## å®‰è£…æ­¥éª¤ï¼ˆWIN10ï¼‰

1. æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ

![image-20200226194910463](./image-20200226194910463.png)

2. è¿›å…¥Anaconda Prompt å®‰è£… tensorflow2.0.0
3. 





## ç§‘å­¦è®¡ç®—å¸¸ç”¨åŒ…



````
pip install keras pandas jupyter matplotlib numpy & pip install sklearn -i  https://pypi.douban.com/simple/
````

å¸¸è§é”™è¯¯ Blas GEMM launch failed: 

å¯é€šè¿‡`RUN nvidia-smi`æ£€æŸ¥æ˜¯å¦æœ‰æ— å…¶ä»–ç¨‹åºåœ¨ä½¿ç”¨CUDAï¼Œæœ‰åˆ™å…³é—­å³å¯x [xin]







# ç¼–ç¨‹å­¦ä¹ å»ºè®®

Generatorå­¦ä¹ ä»¥åŠéå†  åœ¨Dive into DL 3.2  ç¼–ç ä¸­æœ‰è®²

æœ€åŸºç¡€çš„ä¸€ä¸ªkerasçš„å•å±‚æ„ŸçŸ¥æœºè®­ç»ƒçš„æ•°æ®åˆ’åˆ†ã€æ¨¡å‹æ­å»ºã€ä¼˜åŒ–å™¨é€‰æ‹©ã€æŸå¤±å‡½æ•°ã€ä»¥åŠæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å¦‚Dive Into DL 3.3ä¸­æœ‰è®²ã€‚





[SoftMaxå­¦ä¹ æ¦‚è§ˆ](https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter03_DL-basics/3.4_softmax-regression)

æ³¨æ„è§‚å¯Ÿçš„ä»–çš„æ•°æ®çŸ¢é‡è¡¨è¾¾ï¼Œä¸ä¸€èˆ¬è®¤çŸ¥ï¼ˆæˆ‘ï¼‰çš„ä¸åŒï¼Œå³X*Wï¼Œ



![image-20200309195110049](Keraså’ŒTensorFlowçš„æŒ‡å—.assets/image-20200309195110049.png)

è¿™ç§å˜åŒ–è§£å†³äº†å¤šä¸ªæ‰¹åº¦è®¡ç®—çš„å‘é‡ä¹˜æ³•çš„é—®é¢˜ã€‚

é‚£ä¹ˆå°±æœ‰

![image-20200309195524111](./image-20200309195524111.png)

æ³¨æ„è§‚å¯ŸSotfmaxæ˜¯å¯¹yçš„åŒä¸€è¡Œçš„æ•°æ®åšäº†è¿ç®—ã€‚



ä½œè€…å¯¹äºæŸå¤±å‡½æ•°çš„æ€è€ƒï¼š å¦‚æœä½¿ç”¨å¹³æ–¹æŸå¤±å‡½æ•°ï¼Œå¯èƒ½å¯¹äºè®¡ç®—ç»“æœè¿‡äºä¸¥è‹›ï¼Œä¸€ä¸ªæœ‰æ•ˆçš„è®¡ç®—æ–¹å¼å°±æ˜¯äº¤å‰ç†µã€‚

![image-20200309195945288](./image-20200309195945288.png)





## dropoutå®Œæ•´å®ç°

!!

```python
import tensorflow as tf
import numpy as np
from tensorflow import keras, nnï¼Œ losses
from tensorflow.keras.layers import Dropout, Flatten, Dense

def dropout(X, drop_prob):
    assert 0 <= drop_prob <= 1
    keep_prob = 1 - drop_prob
    # è¿™ç§æƒ…å†µä¸‹æŠŠå…¨éƒ¨å…ƒç´ éƒ½ä¸¢å¼ƒ
    if keep_prob == 0:
        return tf.zeros_like(X)
    #åˆå§‹maskä¸ºä¸€ä¸ªboolå‹æ•°ç»„ï¼Œæ•…éœ€è¦å¼ºåˆ¶ç±»å‹è½¬æ¢
    mask = tf.random.uniform(shape=X.shape, minval=0, maxval=1) < keep_prob
    return tf.cast(mask, dtype=tf.float32) * tf.cast(X, dtype=tf.float32) / keep_prob


drop_prob1, drop_prob2 = 0.2, 0.5
#å»ºç«‹ä¸€ä¸ªä¸‰å±‚æ„ŸçŸ¥æœº
def net(X, is_training=False):
    X = tf.reshape(X, shape=(-1,num_inputs))
    H1 = tf.nn.relu(tf.matmul(X, W1) + b1)
    if is_training:# åªåœ¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨ä¸¢å¼ƒæ³•
      H1 = dropout(H1, drop_prob1)  # åœ¨ç¬¬ä¸€å±‚å…¨è¿æ¥åæ·»åŠ ä¸¢å¼ƒå±‚
    H2 = nn.relu(tf.matmul(H1, W2) + b2)
    if is_training:
      H2 = dropout(H2, drop_prob2)  # åœ¨ç¬¬äºŒå±‚å…¨è¿æ¥åæ·»åŠ ä¸¢å¼ƒå±‚
    return tf.math.softmax(tf.matmul(H2, W3) + b3)

```





## åå‘ä¼ æ’­çš„ä¸€ä¸ªè¯´æ˜

[3.14.3 åå‘ä¼ æ’­](https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter03_DL-basics/3.14_backprop?id=_3143-åå‘ä¼ æ’­)

!!



## [3.16 å®æˆ˜Kaggleæ¯”èµ›ï¼šæˆ¿ä»·é¢„æµ‹](https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter03_DL-basics/3.16_kaggle-house-price?id=_316-å®æˆ˜kaggleæ¯”èµ›ï¼šæˆ¿ä»·é¢„æµ‹)  

!!



## [å»¶æ—¶åˆå§‹åŒ–](https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter04_DL-computation/4.3_deferred-init)

!! 





NLP å­¦ä¹   [6.1 è¯­è¨€æ¨¡å‹](https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter06_RNN/6.1_lang-model?id=_61-è¯­è¨€æ¨¡å‹)  

!!





## å›å½’è¯„ä¼°æŒ‡æ ‡

ç…§æŠ„[è¿™ç¯‡](<https://blog.csdn.net/chao2016/article/details/84960257>)



å…¶ä»–è¯¦ç»†æŒ‡æ ‡å¯å‚è€ƒ[è¿™ç¯‡](<https://blog.csdn.net/qq_36962569/article/details/79881065>)

#### å‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼ŒMSEï¼‰

  è§‚æµ‹å€¼ä¸çœŸå€¼åå·®çš„å¹³æ–¹å’Œä¸è§‚æµ‹æ¬¡æ•°çš„æ¯”å€¼ï¼š

![1583999476079](Keraså’ŒTensorFlowçš„æŒ‡å—.assets/1583999476079.png)


è¿™å°±æ˜¯çº¿æ€§å›å½’ä¸­æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼Œçº¿æ€§å›å½’è¿‡ç¨‹ä¸­å°½é‡è®©è¯¥æŸå¤±å‡½æ•°æœ€å°ã€‚é‚£ä¹ˆæ¨¡å‹ä¹‹é—´çš„å¯¹æ¯”ä¹Ÿå¯ä»¥ç”¨å®ƒæ¥æ¯”è¾ƒã€‚MSEå¯ä»¥è¯„ä»·æ•°æ®çš„å˜åŒ–ç¨‹åº¦ï¼ŒMSEçš„å€¼è¶Šå°ï¼Œè¯´æ˜é¢„æµ‹æ¨¡å‹æè¿°å®éªŒæ•°æ®å…·æœ‰æ›´å¥½çš„ç²¾ç¡®åº¦ã€‚

#### å‡æ–¹æ ¹è¯¯å·®ï¼ˆæ ‡å‡†è¯¯å·®ï¼‰ï¼ˆRoot Mean Squard Errorï¼ŒRMSEï¼‰

  æ ‡å‡†å·®æ˜¯æ–¹å·®çš„ç®—æœ¯å¹³æ–¹æ ¹ã€‚æ ‡å‡†è¯¯å·®æ˜¯å‡æ–¹è¯¯å·®çš„ç®—æœ¯å¹³æ–¹æ ¹ã€‚
  æ ‡å‡†å·®æ˜¯ç”¨æ¥è¡¡é‡ä¸€ç»„æ•°è‡ªèº«çš„ç¦»æ•£ç¨‹åº¦ï¼Œè€Œå‡æ–¹æ ¹è¯¯å·®æ˜¯ç”¨æ¥è¡¡é‡è§‚æµ‹å€¼åŒçœŸå€¼ä¹‹é—´çš„åå·®ï¼Œå®ƒä»¬çš„ç ”ç©¶å¯¹è±¡å’Œç ”ç©¶ç›®çš„ä¸åŒï¼Œä½†æ˜¯è®¡ç®—è¿‡ç¨‹ç±»ä¼¼ã€‚

![1583999552175](Keraså’ŒTensorFlowçš„æŒ‡å—.assets/1583999552175.png)


å®ƒçš„æ„ä¹‰åœ¨äºå¼€ä¸ªæ ¹å·åï¼Œè¯¯å·®çš„ç»“æœå°±ä¸æ•°æ®æ˜¯ä¸€ä¸ªçº§åˆ«çš„ï¼Œå¯ä»¥æ›´å¥½åœ°æ¥æè¿°æ•°æ®ã€‚æ ‡å‡†è¯¯å·®å¯¹ä¸€ç»„æµ‹é‡ä¸­çš„ç‰¹å¤§æˆ–ç‰¹å°è¯¯å·®åæ˜ éå¸¸æ•æ„Ÿï¼Œæ‰€ä»¥ï¼Œæ ‡å‡†è¯¯å·®èƒ½å¤Ÿå¾ˆå¥½åœ°åæ˜ å‡ºæµ‹é‡çš„ç²¾å¯†åº¦ã€‚è¿™æ­£æ˜¯æ ‡å‡†è¯¯å·®åœ¨å·¥ç¨‹æµ‹é‡ä¸­å¹¿æ³›è¢«é‡‡ç”¨çš„åŸå› ã€‚

#### å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean Absolute Errorï¼ŒMAEï¼‰

å¹³å‡ç»å¯¹è¯¯å·®æ˜¯ç»å¯¹è¯¯å·®çš„å¹³å‡å€¼ ï¼š

![1583999588663](Keraså’ŒTensorFlowçš„æŒ‡å—.assets/1583999588663.png)

å¹³å‡ç»å¯¹è¯¯å·®èƒ½æ›´å¥½åœ°åæ˜ é¢„æµ‹å€¼è¯¯å·®çš„å®é™…æƒ…å†µ.

#### R-squared



![1583999609952](Keraså’ŒTensorFlowçš„æŒ‡å—.assets/1583999609952.png)

ä¸Šé¢åˆ†å­å°±æ˜¯æˆ‘ä»¬è®­ç»ƒå‡ºçš„æ¨¡å‹é¢„æµ‹çš„è¯¯å·®å’Œã€‚
ä¸‹é¢åˆ†æ¯å°±æ˜¯å–å¹³å‡ç­–ç•¥çš„è¯¯å·®å’Œã€‚ï¼ˆé€šå¸¸å–è§‚æµ‹å€¼çš„å¹³å‡å€¼ï¼‰

å¦‚æœç»“æœæ˜¯0ï¼Œå°±è¯´æ˜æˆ‘ä»¬çš„æ¨¡å‹è·ŸççŒœå·®ä¸å¤šã€‚
å¦‚æœç»“æœæ˜¯1ã€‚å°±è¯´æ˜æˆ‘ä»¬æ¨¡å‹æ— é”™è¯¯ã€‚

$R^2$ ä»‹äº0~1ä¹‹é—´ï¼Œè¶Šæ¥è¿‘1ï¼Œå›å½’æ‹Ÿåˆæ•ˆæœè¶Šå¥½ï¼Œä¸€èˆ¬è®¤ä¸ºè¶…è¿‡0.8çš„æ¨¡å‹æ‹Ÿåˆä¼˜åº¦æ¯”è¾ƒé«˜ã€‚

åŒ–ç®€ä¸Šé¢çš„å…¬å¼
åˆ†å­åˆ†æ¯åŒæ—¶é™¤ä»¥mï¼Œé‚£ä¹ˆåˆ†å­å°±å˜æˆäº†æˆ‘ä»¬çš„å‡æ–¹è¯¯å·®MSEï¼Œä¸‹é¢åˆ†æ¯å°±å˜æˆäº†æ–¹å·®ã€‚

![1583999663089](Keraså’ŒTensorFlowçš„æŒ‡å—.assets/1583999663089.png)



### Pythonå®ç°

$MSE$

```python
def rmse(y_test, y_true):
    return sp.mean((y_test - y_true) ** 2)
```

$RMSE$

```python
def rmse(y_test, y_true):
    return sp.sqrt(sp.mean((y_test - y_true) ** 2))
```


$MAE$

```python
def mae(y_test, y_true):
    return np.sum(np.absolute(y_test - y_true)) / len(y_test)

```



$R^2$

```python
def r2(y_test, y_true):
    return 1 - ((y_test - y_true) ** 2).sum() / ((y_true - np.mean(y_true)) ** 2).sum()

```

sklearnçš„è°ƒç”¨

```python
from sklearn.metrics import mean_squared_error 
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score

mean_squared_error(y_test,y_predict)
mean_absolute_error(y_test,y_predict)
r2_score(y_test,y_predict)
```





